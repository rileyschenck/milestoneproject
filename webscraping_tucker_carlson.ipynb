{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install --upgrade pip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install html5lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import urllib.request\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# establish selenium driver to click through the 'load more' buttom \n",
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://www.foxnews.com/category/shows/tucker-carlson-tonight/transcript\")\n",
    "button = driver.find_element(By.XPATH, \"//div[@class='button load-more js-load-more']\") \n",
    "\n",
    "for x in range(50):\n",
    "    button.click()    \n",
    "    #time.sleep(1)\n",
    "    \n",
    "# save the source to feed to beautiful soup \n",
    "page_source = driver.page_source \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(page_source)\n",
    "url_soup = soup.find_all('a', href=re.compile('/opinion/.*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#url_soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of urls for the transcript webpages \n",
    "\n",
    "url_list = []\n",
    "prefix = 'https://www.foxnews.com/'\n",
    "for link in url_soup: \n",
    "    suffix = re.findall('\\\"\\/(.*?)\\\"', str(link))\n",
    "    if suffix:\n",
    "        url_list.append(prefix + suffix[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use BeautifulSoup to webscrape each Tucker Carlson webpage and store the output in dfs\n",
    "\n",
    "full_df = None\n",
    "dfs = []\n",
    "\n",
    "for n, url in enumerate(url_list):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    time = soup.find('time').text\n",
    "    title = soup.title.string\n",
    "\n",
    "    a_tag = soup.find_all('a')\n",
    "    for tag in a_tag:\n",
    "        tag.decompose()\n",
    "\n",
    "    copy_tag = soup.find_all('p', class_ = \"copyright\")\n",
    "    for tag in copy_tag:\n",
    "        tag.decompose()\n",
    "\n",
    "    span_tag = soup.find_all('span')\n",
    "    for tag in span_tag:\n",
    "        tag.decompose()\n",
    "\n",
    "    bold_tag = soup.find_all('strong') \n",
    "    for tag in bold_tag:\n",
    "        tag.decompose()\n",
    "\n",
    "    italic_tag=soup.find_all('i') \n",
    "    for tag in italic_tag:\n",
    "        tag.decompose()\n",
    "\n",
    "    other_tag = soup.find_all('p', class_ =\"dek\")\n",
    "    for tag in other_tag:\n",
    "        tag.decompose()\n",
    "\n",
    "    other_tag2 = soup.find_all('p', class_ =\"subscribed hide\")\n",
    "    for tag in other_tag2:\n",
    "        tag.decompose()\n",
    "\n",
    "    other_tag3 = soup.find_all('p', class_ =\"success hide\") \n",
    "    for tag in other_tag3:\n",
    "        tag.decompose()\n",
    "\n",
    "\n",
    "    final = soup.find_all('p')\n",
    "    # text=''\n",
    "    # print(final)\n",
    "    finaltext = ''\n",
    "    for p in final:\n",
    "\n",
    "        try:\n",
    "            finaltext = finaltext + p.text\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    data={'url': [url],'timestamp': [time], 'title': [title], 'transcript': [finaltext]}\n",
    "    df = pd.DataFrame(data)\n",
    "    dfs.append(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = pd.concat(dfs).reset_index(drop=True)\n",
    "#full_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store output\n",
    "full_df.to_csv('Tucker_transcripts.tsv', sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
